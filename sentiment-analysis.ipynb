{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.46.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (2.28.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (0.20.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (4.64.1)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (1.1.0)\n",
      "Requirement already satisfied: torch in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (1.12.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[torch]) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[torch]) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[torch]) (2022.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: shuup 3.1.0 has a non-standard dependency specifier toml<1pytz>=2015.4,>=0.10.0. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of shuup or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit_learn in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit_learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit_learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit_learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit_learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: shuup 3.1.0 has a non-standard dependency specifier toml<1pytz>=2015.4,>=0.10.0. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of shuup or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "DEPRECATION: shuup 3.1.0 has a non-standard dependency specifier toml<1pytz>=2015.4,>=0.10.0. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of shuup or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: torch in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.12.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.46.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\toktarla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: shuup 3.1.0 has a non-standard dependency specifier toml<1pytz>=2015.4,>=0.10.0. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of shuup or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit_learn\n",
    "!pip install transformers[torch]\n",
    "!pip install accelerate>={ACCELERATE_MIN_VERSION}\n",
    "!pip install pandas torch transformers scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EvalPrediction, pipeline\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Environment Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Compute Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred: EvalPrediction):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)  # Get the predicted class indices\n",
    "\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')  # or 'macro' based on your preference\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      custom_id                                     text  \\\n",
      "0  pla015439pla                               Өтте күшті   \n",
      "1  pla083193pla  Мәбазар жок .Оте керемет тамаша керемет   \n",
      "2  pla113624pla             Кушти , дал тура айтады 👍👍👍👍   \n",
      "3  pla029825pla                              Реклама коп   \n",
      "4  pla002604pla                           5-баға беремін   \n",
      "\n",
      "                             text_cleaned  label    domain  \n",
      "0                              өтте күшті      1  appstore  \n",
      "1  мәбазар жок оте керемет тамаша керемет      1  appstore  \n",
      "2                   кушти дал тура айтады      1  appstore  \n",
      "3                             реклама коп      0  appstore  \n",
      "4                          5 баға беремін      1  appstore  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('01_pc_train_ib.csv')\n",
    "\n",
    "# Show the first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load BERT Tokenizer and Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the multilingual BERT tokenizer and model\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Custom Dataset Class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KazakhSentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['text_cleaned'].tolist() \n",
    "labels = df['label'].tolist()\n",
    "\n",
    "# Create dataset and split it into training and validation sets\n",
    "dataset = KazakhSentimentDataset(texts, labels, tokenizer)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='C:/Users/Toktarla/OneDrive/Desktop/sentiment-analysis-kz/results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=16,  \n",
    "    gradient_accumulation_steps=1,  \n",
    "    logging_dir='C:/Users/Toktarla/OneDrive/Desktop/sentiment-analysis-kz/logs',\n",
    "    logging_steps=50,  \n",
    "    warmup_steps=300,  \n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",  \n",
    "    save_total_limit=2,  \n",
    "    eval_strategy=\"epoch\", \n",
    "    learning_rate=3e-5, \n",
    "    fp16=torch.cuda.is_available(), \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Trainer and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829e67675cbf4d53b3fc45f8e3cf9cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Toktarla\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\amp\\autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9253, 'grad_norm': 3.558798313140869, 'learning_rate': 4.9999999999999996e-06, 'epoch': 0.2}\n",
      "{'loss': 0.4872, 'grad_norm': 2.562490701675415, 'learning_rate': 9.999999999999999e-06, 'epoch': 0.4}\n",
      "{'loss': 0.4623, 'grad_norm': 4.197021961212158, 'learning_rate': 1.5e-05, 'epoch': 0.6}\n",
      "{'loss': 0.4078, 'grad_norm': 4.171961307525635, 'learning_rate': 1.9999999999999998e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf2595cb972411b9e1d16bb0b61629c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36107271909713745, 'eval_accuracy': 0.8568548387096774, 'eval_f1': 0.8411748917968487, 'eval_precision': 0.8424289413051333, 'eval_recall': 0.8568548387096774, 'eval_runtime': 241.1936, 'eval_samples_per_second': 4.113, 'eval_steps_per_second': 0.257, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Toktarla\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\amp\\autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3914, 'grad_norm': 10.131441116333008, 'learning_rate': 2.5e-05, 'epoch': 1.01}\n",
      "{'loss': 0.2603, 'grad_norm': 5.560835361480713, 'learning_rate': 3e-05, 'epoch': 1.21}\n",
      "{'loss': 0.3958, 'grad_norm': 4.12003231048584, 'learning_rate': 2.6621621621621624e-05, 'epoch': 1.41}\n",
      "{'loss': 0.3821, 'grad_norm': 3.147355318069458, 'learning_rate': 2.3243243243243243e-05, 'epoch': 1.61}\n",
      "{'loss': 0.324, 'grad_norm': 5.962643146514893, 'learning_rate': 1.9864864864864866e-05, 'epoch': 1.81}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ce78bfebe94610afa27932fafe76f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3675060272216797, 'eval_accuracy': 0.8578629032258065, 'eval_f1': 0.8488942899295825, 'eval_precision': 0.8462416568007574, 'eval_recall': 0.8578629032258065, 'eval_runtime': 235.0033, 'eval_samples_per_second': 4.221, 'eval_steps_per_second': 0.264, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Toktarla\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\amp\\autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2785, 'grad_norm': 5.691867351531982, 'learning_rate': 1.648648648648649e-05, 'epoch': 2.02}\n",
      "{'loss': 0.2412, 'grad_norm': 5.426058292388916, 'learning_rate': 1.3108108108108109e-05, 'epoch': 2.22}\n",
      "{'loss': 0.2792, 'grad_norm': 1.2105828523635864, 'learning_rate': 9.72972972972973e-06, 'epoch': 2.42}\n",
      "{'loss': 0.2511, 'grad_norm': 7.025335788726807, 'learning_rate': 6.351351351351352e-06, 'epoch': 2.62}\n",
      "{'loss': 0.2119, 'grad_norm': 6.159411430358887, 'learning_rate': 2.972972972972973e-06, 'epoch': 2.82}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f3b0ee41d04ca1851dfb9b49662d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4542233943939209, 'eval_accuracy': 0.8518145161290323, 'eval_f1': 0.8424642597138201, 'eval_precision': 0.8392516024279031, 'eval_recall': 0.8518145161290323, 'eval_runtime': 217.429, 'eval_samples_per_second': 4.562, 'eval_steps_per_second': 0.285, 'epoch': 3.0}\n",
      "{'train_runtime': 13639.5844, 'train_samples_per_second': 0.873, 'train_steps_per_second': 0.055, 'train_loss': 0.36684104447723714, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=744, training_loss=0.36684104447723714, metrics={'train_runtime': 13639.5844, 'train_samples_per_second': 0.873, 'train_steps_per_second': 0.055, 'total_flos': 783025531158528.0, 'train_loss': 0.36684104447723714, 'epoch': 3.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Create a Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4542233943939209, 'eval_accuracy': 0.8518145161290323, 'eval_f1': 0.8424642597138201, 'eval_precision': 0.8392516024279031, 'eval_recall': 0.8518145161290323, 'eval_runtime': 274.0429, 'eval_samples_per_second': 3.62, 'eval_steps_per_second': 0.226, 'epoch': 3.0}\n",
      "Validation Accuracy: 0.8518145161290323\n",
      "Validation F1 Score: 0.8424642597138201\n",
      "Validation Precision: 0.8392516024279031\n",
      "Validation Recall: 0.8518145161290323\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "eval_result = trainer.evaluate()\n",
    "\n",
    "# Access the evaluation results\n",
    "accuracy = eval_result.get('eval_accuracy', eval_result.get('accuracy'))\n",
    "f1 = eval_result.get('eval_f1', eval_result.get('f1'))\n",
    "precision = eval_result.get('eval_precision', eval_result.get('precision'))\n",
    "recall = eval_result.get('eval_recall', eval_result.get('recall'))\n",
    "\n",
    "print(eval_result)\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "print(f\"Validation F1 Score: {f1}\")\n",
    "print(f\"Validation Precision: {precision}\")\n",
    "print(f\"Validation Recall: {recall}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero Shot Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.54\n",
      "F1 Score: 0.6088565488565488\n",
      "Precision: 0.7184719864176572\n",
      "Recall: 0.54\n",
      "\n",
      "Example Predictions with Confidence Scores:\n",
      "Text: көп орын алмайды дизайны жақсы жеңіл жақсы суытады екі жақ жаны ыстық болып тұрады үнемі\n",
      "Prediction: 1 (positive), Confidence: 0.5024\n",
      "--------------------------------------------------\n",
      "Text: өте керемет фонды істеу\n",
      "Prediction: 0 (negative), Confidence: 0.5004\n",
      "--------------------------------------------------\n",
      "Text: бұл праграмма өте өте қатты ұнады\n",
      "Prediction: 1 (positive), Confidence: 0.5007\n",
      "--------------------------------------------------\n",
      "Text: тамак заказ бердык бырак сан сапасы жок соны айтсак шешып бере алмады\n",
      "Prediction: 0 (negative), Confidence: 0.5003\n",
      "--------------------------------------------------\n",
      "Text: бетті қалай ауыстырады\n",
      "Prediction: 1 (positive), Confidence: 0.5043\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"bert-base-multilingual-cased\")\n",
    "\n",
    "candidate_labels = [\"positive\", \"negative\"]\n",
    "\n",
    "def zero_shot_predict(text):\n",
    "    result = zero_shot_classifier(text, candidate_labels)\n",
    "    highest_label = result['labels'][0]\n",
    "    highest_score = result['scores'][0]\n",
    "    return highest_label, highest_score\n",
    "\n",
    "def evaluate_zero_shot_model(df):\n",
    "    true_labels = df['label'].tolist()  # Assuming the true labels are in the 'label' column\n",
    "    predictions = []\n",
    "    prediction_scores = []\n",
    "\n",
    "    for text in df['text_cleaned'].tolist():  # Assuming 'text_cleaned' contains the texts\n",
    "        prediction, score = zero_shot_predict(text)\n",
    "        predicted_label = 0 if prediction == \"negative\" else 1\n",
    "        predictions.append(predicted_label)\n",
    "        prediction_scores.append(score)\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    precision = precision_score(true_labels, predictions, average='weighted')\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "\n",
    "    print(\"\\nExample Predictions with Confidence Scores:\")\n",
    "    for i in range(5):  # Display first 5 predictions\n",
    "        print(f\"Text: {df['text_cleaned'].iloc[i]}\")\n",
    "        print(f\"Prediction: {predictions[i]} ({'positive' if predictions[i] == 1 else 'negative'}), Confidence: {prediction_scores[i]:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "df_sample = df.sample(n=100, random_state=42)  # random_state ensures reproducibility\n",
    "\n",
    "evaluate_zero_shot_model(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dcba26a08cb449b92c451fc2a9b2e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60d7c2d468e4242bb460f46234569cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Toktarla\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5710974335670471, 'eval_accuracy': 0.85, 'eval_f1': 0.7810810810810811, 'eval_precision': 0.7224999999999999, 'eval_recall': 0.85, 'eval_runtime': 5.2506, 'eval_samples_per_second': 3.809, 'eval_steps_per_second': 0.381, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e71ef993253493aa0748b91d6e8ac64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Toktarla\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5019818544387817, 'eval_accuracy': 0.85, 'eval_f1': 0.7810810810810811, 'eval_precision': 0.7224999999999999, 'eval_recall': 0.85, 'eval_runtime': 5.0396, 'eval_samples_per_second': 3.969, 'eval_steps_per_second': 0.397, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59ce1bc862043be8480f73bd76f67f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Toktarla\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4457380771636963, 'eval_accuracy': 0.85, 'eval_f1': 0.7810810810810811, 'eval_precision': 0.7224999999999999, 'eval_recall': 0.85, 'eval_runtime': 5.9148, 'eval_samples_per_second': 3.381, 'eval_steps_per_second': 0.338, 'epoch': 3.0}\n",
      "{'train_runtime': 243.6033, 'train_samples_per_second': 0.985, 'train_steps_per_second': 0.062, 'train_loss': 0.5294204076131185, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f9b475da774266a48ad30788a03789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4457380771636963, 'eval_accuracy': 0.85, 'eval_f1': 0.7810810810810811, 'eval_precision': 0.7224999999999999, 'eval_recall': 0.85, 'eval_runtime': 5.2951, 'eval_samples_per_second': 3.777, 'eval_steps_per_second': 0.378, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Toktarla\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model_few_shot = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "few_shot_train_size = 100  \n",
    "\n",
    "# Select a few-shot subset of the dataset\n",
    "few_shot_df = df.sample(n=few_shot_train_size, random_state=42)\n",
    "\n",
    "# Prepare the dataset (same as before)\n",
    "texts_few_shot = few_shot_df['text_cleaned'].tolist()\n",
    "labels_few_shot = few_shot_df['label'].tolist()\n",
    "\n",
    "# Create a smaller dataset\n",
    "train_dataset_few_shot = KazakhSentimentDataset(texts_few_shot, labels_few_shot, tokenizer)\n",
    "\n",
    "# Split the few-shot data into train and validation sets\n",
    "train_size_few_shot = int(0.8 * len(train_dataset_few_shot))\n",
    "val_size_few_shot = len(train_dataset_few_shot) - train_size_few_shot\n",
    "train_dataset_few_shot, val_dataset_few_shot = random_split(train_dataset_few_shot, [train_size_few_shot, val_size_few_shot])\n",
    "\n",
    "# Update training arguments for a faster few-shot training process\n",
    "few_shot_training_args = TrainingArguments(\n",
    "    output_dir='C:/Users/Toktarla/OneDrive/Desktop/sentiment-analysis-kz/results_few_shot',\n",
    "    num_train_epochs=3,  # Train with few epochs for faster learning\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    logging_dir='C:/Users/Toktarla/OneDrive/Desktop/sentiment-analysis-kz/logs_few_shot',\n",
    "    warmup_steps=50,  # Adjust warmup for faster convergence\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Create a Trainer for few-shot learning\n",
    "trainer_few_shot = Trainer(\n",
    "    model=model_few_shot,\n",
    "    args=few_shot_training_args,\n",
    "    train_dataset=train_dataset_few_shot,\n",
    "    eval_dataset=val_dataset_few_shot,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Fine-tune the model on the few-shot dataset\n",
    "trainer_few_shot.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7418ca861cb6413ebfea165871374d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4457380771636963, 'eval_accuracy': 0.85, 'eval_f1': 0.7810810810810811, 'eval_precision': 0.7224999999999999, 'eval_recall': 0.85, 'eval_runtime': 5.5298, 'eval_samples_per_second': 3.617, 'eval_steps_per_second': 0.362, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Toktarla\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "eval_result_few_shot = trainer_few_shot.evaluate()\n",
    "print(eval_result_few_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "\n",
    "    output = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = output.logits\n",
    "\n",
    "    prediction = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    return prediction  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Prediction with Sample Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: 0\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Я ниче не говорю про суши.\"\n",
    "print(f\"Predicted Sentiment: {predict_sentiment(sample_text)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
